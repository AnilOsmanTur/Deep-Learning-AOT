{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Network for Regression (Estimate blood pressure from PPG signal)\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [HW page](http://kovan.ceng.metu.edu.tr/~sinan/DL/index.html) on the course website.*\n",
    "\n",
    "Having gained some experience with neural networks, let us train a network that estimates the blood pressure from a PPG signal window.\n",
    "\n",
    "All of your work for this exercise will be done in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Photoplethysmograph (PPG) signal\n",
    "\n",
    "A PPG (photoplethysmograph) signal is a signal obtained with a pulse oximeter, which illuminates the skin and measures changes in light absorption. A PPG signal carries rich information about the status of the cardiovascular health of a person, such as breadth rate, heart rate and blood pressure. An example is shown below, where you also see the blood pressure signal that we will estimate (the data also has the ECG signal, which you should ignore).\n",
    "\n",
    "<img width=\"80%\" src=\"PPG_ABG_ECG_example.png\">\n",
    "\n",
    "\n",
    "# Constructing the Dataset \n",
    "\n",
    "In this task, you are expected to perform the full pipeline for creating a learning system from scratch. Here is how you should construct the dataset:\n",
    "* Download the dataset from the following website, and only take \"Part 1\" from it (it is too big): https://archive.ics.uci.edu/ml/datasets/Cuff-Less+Blood+Pressure+Estimation\n",
    "* Take a window of size $W$ from the PPG channel between time $t$ and $t+W$. Let us call this $\\textbf{x}_t$.\n",
    "* Take the corresponding window of size $W$ from the ABP (arterial blood pressure) channel between time $t$ and $t+W$. Find the maxima and minima of this signal within the window (you can use \"findpeaks\" from Matlab or \"find_peaks_cwt\" from scipy). Here is an example window from the ABP signal, and its peaks:\n",
    " <img width=\"60%\" src=\"ABP_peaks.png\">\n",
    "    \n",
    "* Calculate the average of the maxima, call it $y^1_t$, and the average of the minima, call it $y^2_t$.\n",
    "* Slide the window over the PPG signals and collect many $(\\textbf{x}_t, <y^1_t, y^2_t>)$ instances. In other words, your network outputs two values.\n",
    "* This will be your input-output for training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from metu.data_utils import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "from cs231n.classifiers.neural_net_for_regression import TwoLayerNet\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([[0, 1, 2], [1, 2, 3], [2, 3, 4], [2, 1, 4], [2, 1, 4]])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass: compute scores\n",
    "Open the file `cs231n/classifiers/neural_net_for_regression.py` and look at the method `TwoLayerNet.loss`. This function is very similar to the loss functions you have written for the previous exercises: It takes the data and weights and computes the *regression* scores, the squared error loss, and the gradients on the parameters. \n",
    "\n",
    "To be more specific, you will implement the following loss function:\n",
    "\n",
    "$$\\frac{1}{2}\\sum_i\\sum_{j} (o_{ij} - y_{ij})^2 + \\frac{1}{2}\\lambda\\sum_j w_j^2,$$\n",
    "\n",
    "where $i$ runs through the samples in the batch; $o_{ij}$ is the prediction of the network for the $i^{th}$ sample for output $j$, and $y_{ij}$ is the correct value; $\\lambda$ is the weight of the regularization term.\n",
    "\n",
    "The first layer uses ReLU as the activation function. The output layer does not use any activation functions.\n",
    "\n",
    "Implement the first part of the forward pass which uses the weights and biases to compute the scores for all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "correct scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "3.68027209324e-08\n"
     ]
    }
   ],
   "source": [
    "scores = net.loss(X)\n",
    "print ('Your scores:')\n",
    "print (scores)\n",
    "print('')\n",
    "print ('correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print (correct_scores)\n",
    "print('')\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print ('Difference between your scores and correct scores:')\n",
    "print (np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass: compute loss\n",
    "In the same function, implement the second part that computes the data and regularizaion loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 66.3406756909\n",
      "Difference between your loss and correct loss:\n",
      "2.54800625044e-11\n"
     ]
    }
   ],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.1)\n",
    "correct_loss = 66.3406756909\n",
    "print('loss:', loss)\n",
    "# should be very small, we get < 1e-10\n",
    "print ('Difference between your loss and correct loss:')\n",
    "print (np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass\n",
    "Implement the rest of the function. This will compute the gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`. Now that you (hopefully!) have a correctly implemented forward pass, you can debug your backward pass using a numeric gradient check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 max relative error: 3.755046e-04\n",
      "b2 max relative error: 1.443387e-06\n",
      "W1 max relative error: 5.463838e-04\n",
      "b1 max relative error: 2.188996e-07\n"
     ]
    }
   ],
   "source": [
    "from cs231n.gradient_check import eval_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.1)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.1)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name])\n",
    "    print ('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the PPG dataset for training your regression network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the training set:  23669\n",
      "Number of instances in the validation set:  263\n",
      "Number of instances in the testing set:  1578\n"
     ]
    }
   ],
   "source": [
    "# Load the PPG dataset\n",
    "# If your memory turns out to be sufficient, try loading a subset\n",
    "def get_data(datafile,\n",
    "             training_ratio=0.9,\n",
    "             test_ratio=0.06,\n",
    "             val_ratio=0.01,\n",
    "             window=input_size,\n",
    "             width_limit=50,\n",
    "             stride=750):\n",
    "    # Load the PPG training data stride\n",
    "    X, y = load_dataset(datafile, window=window, stride=stride, width_limit=width_limit)\n",
    "\n",
    "    # TODO: Split the data into training, validation and test sets\n",
    "    length=len(y)\n",
    "    num_training=int(length*training_ratio)\n",
    "    num_val = int(length*val_ratio)\n",
    "    num_test = min((length-num_training-num_val), int(length*test_ratio))\n",
    "    mask = range(num_training-1)\n",
    "    X_train = X[mask]\n",
    "    y_train = y[mask]\n",
    "    mask = range(num_training, num_training+num_test)\n",
    "    X_test = X[mask]\n",
    "    y_test = y[mask]\n",
    "    mask = range(num_training+num_test, num_training+num_test+num_val)\n",
    "    X_val = X[mask]\n",
    "    y_val = y[mask]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "datafile = './metu/dataset/Part_1.mat' #TODO: PATH to your data file\n",
    "input_size = 1000 # TODO: Size of the input of the network\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_data(datafile, window=input_size, width_limit=50, stride=750)\n",
    "print (\"Number of instances in the training set: \", len(X_train))\n",
    "print (\"Number of instances in the validation set: \", len(X_val))\n",
    "print (\"Number of instances in the testing set: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now train our network on the PPG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 50000: loss 534330.183591\n",
      "iteration 100 / 50000: loss 501336.635259\n",
      "iteration 200 / 50000: loss 451644.055357\n",
      "iteration 300 / 50000: loss 378601.070585\n",
      "iteration 400 / 50000: loss 375677.026417\n",
      "iteration 500 / 50000: loss 331224.788309\n",
      "iteration 600 / 50000: loss 313916.900274\n",
      "iteration 700 / 50000: loss 254183.236685\n",
      "iteration 800 / 50000: loss 211796.817715\n",
      "iteration 900 / 50000: loss 253944.680456\n",
      "iteration 1000 / 50000: loss 206615.596300\n",
      "iteration 1100 / 50000: loss 184450.992006\n",
      "iteration 1200 / 50000: loss 200981.136536\n",
      "iteration 1300 / 50000: loss 153581.951940\n",
      "iteration 1400 / 50000: loss 33338.694818\n",
      "iteration 1500 / 50000: loss 121730.195078\n",
      "iteration 1600 / 50000: loss 93690.350939\n",
      "iteration 1700 / 50000: loss 120235.627992\n",
      "iteration 1800 / 50000: loss 55887.420876\n",
      "iteration 1900 / 50000: loss 37860.012189\n",
      "iteration 2000 / 50000: loss 97783.312405\n",
      "iteration 2100 / 50000: loss 95808.354790\n",
      "iteration 2200 / 50000: loss 91224.020585\n",
      "iteration 2300 / 50000: loss 65411.650035\n",
      "iteration 2400 / 50000: loss 73963.228049\n",
      "iteration 2500 / 50000: loss 100228.298466\n",
      "iteration 2600 / 50000: loss 72141.724880\n",
      "iteration 2700 / 50000: loss 99560.535845\n",
      "iteration 2800 / 50000: loss 68260.499694\n",
      "iteration 2900 / 50000: loss 85566.594876\n",
      "iteration 3000 / 50000: loss 68253.316157\n",
      "iteration 3100 / 50000: loss 47478.798357\n",
      "iteration 3200 / 50000: loss 71682.557370\n",
      "iteration 3300 / 50000: loss 55259.252782\n",
      "iteration 3400 / 50000: loss 43088.090828\n",
      "iteration 3500 / 50000: loss 50131.031938\n",
      "iteration 3600 / 50000: loss 55681.344138\n",
      "iteration 3700 / 50000: loss 53786.701601\n",
      "iteration 3800 / 50000: loss 47674.958703\n",
      "iteration 3900 / 50000: loss 46269.356469\n",
      "iteration 4000 / 50000: loss 41226.722019\n",
      "iteration 4100 / 50000: loss 50456.110290\n",
      "iteration 4200 / 50000: loss 31262.146473\n",
      "iteration 4300 / 50000: loss 52271.646154\n",
      "iteration 4400 / 50000: loss 38009.236974\n",
      "iteration 4500 / 50000: loss 50517.520839\n",
      "iteration 4600 / 50000: loss 48748.319540\n",
      "iteration 4700 / 50000: loss 37883.692506\n",
      "iteration 4800 / 50000: loss 48163.686155\n",
      "iteration 4900 / 50000: loss 38834.417139\n",
      "iteration 5000 / 50000: loss 49833.956842\n",
      "iteration 5100 / 50000: loss 37473.260348\n",
      "iteration 5200 / 50000: loss 22508.918266\n",
      "iteration 5300 / 50000: loss 44819.569245\n",
      "iteration 5400 / 50000: loss 36218.820818\n",
      "iteration 5500 / 50000: loss 35031.128803\n",
      "iteration 5600 / 50000: loss 35442.522311\n",
      "iteration 5700 / 50000: loss 29592.753643\n",
      "iteration 5800 / 50000: loss 37125.209922\n",
      "iteration 5900 / 50000: loss 33071.422018\n",
      "iteration 6000 / 50000: loss 50711.631995\n",
      "iteration 6100 / 50000: loss 36626.956940\n",
      "iteration 6200 / 50000: loss 31963.708785\n",
      "iteration 6300 / 50000: loss 32856.145513\n",
      "iteration 6400 / 50000: loss 36583.528925\n",
      "iteration 6500 / 50000: loss 31920.863215\n",
      "iteration 6600 / 50000: loss 30206.229017\n",
      "iteration 6700 / 50000: loss 34701.552415\n",
      "iteration 6800 / 50000: loss 37768.558471\n",
      "iteration 6900 / 50000: loss 35069.716678\n",
      "iteration 7000 / 50000: loss 38239.992569\n",
      "iteration 7100 / 50000: loss 30433.656583\n",
      "iteration 7200 / 50000: loss 35668.422686\n",
      "iteration 7300 / 50000: loss 36894.342225\n",
      "iteration 7400 / 50000: loss 30212.682203\n",
      "iteration 7500 / 50000: loss 39973.608519\n",
      "iteration 7600 / 50000: loss 33090.373128\n",
      "iteration 7700 / 50000: loss 30982.045499\n",
      "iteration 7800 / 50000: loss 28433.581688\n",
      "iteration 7900 / 50000: loss 30570.078456\n",
      "iteration 8000 / 50000: loss 34023.266008\n",
      "iteration 8100 / 50000: loss 30313.859017\n",
      "iteration 8200 / 50000: loss 30403.643335\n",
      "iteration 8300 / 50000: loss 32669.630737\n",
      "iteration 8400 / 50000: loss 32125.770980\n",
      "iteration 8500 / 50000: loss 32562.270506\n",
      "iteration 8600 / 50000: loss 26260.541767\n",
      "iteration 8700 / 50000: loss 30969.072655\n",
      "iteration 8800 / 50000: loss 35427.100290\n",
      "iteration 8900 / 50000: loss 30691.190901\n",
      "iteration 9000 / 50000: loss 28351.775023\n",
      "iteration 9100 / 50000: loss 32346.229702\n",
      "iteration 9200 / 50000: loss 33794.071304\n",
      "iteration 9300 / 50000: loss 39634.648919\n",
      "iteration 9400 / 50000: loss 29765.376633\n",
      "iteration 9500 / 50000: loss 29280.737959\n",
      "iteration 9600 / 50000: loss 43467.543374\n",
      "iteration 9700 / 50000: loss 25916.221347\n",
      "iteration 9800 / 50000: loss 29189.679520\n",
      "iteration 9900 / 50000: loss 33440.527503\n",
      "iteration 10000 / 50000: loss 32557.160264\n",
      "iteration 10100 / 50000: loss 30727.421866\n",
      "iteration 10200 / 50000: loss 39153.886111\n",
      "iteration 10300 / 50000: loss 33413.371650\n",
      "iteration 10400 / 50000: loss 35425.779166\n",
      "iteration 10500 / 50000: loss 30976.999230\n",
      "iteration 10600 / 50000: loss 33930.644984\n",
      "iteration 10700 / 50000: loss 36418.912151\n",
      "iteration 10800 / 50000: loss 30481.569765\n",
      "iteration 10900 / 50000: loss 23265.772910\n",
      "iteration 11000 / 50000: loss 29129.651268\n",
      "iteration 11100 / 50000: loss 34956.172549\n",
      "iteration 11200 / 50000: loss 34017.697496\n",
      "iteration 11300 / 50000: loss 32684.313722\n",
      "iteration 11400 / 50000: loss 29571.925161\n",
      "iteration 11500 / 50000: loss 24462.671804\n",
      "iteration 11600 / 50000: loss 22032.304924\n",
      "iteration 11700 / 50000: loss 29409.501127\n",
      "iteration 11800 / 50000: loss 27664.374866\n",
      "iteration 11900 / 50000: loss 29461.826129\n",
      "iteration 12000 / 50000: loss 26028.597995\n",
      "iteration 12100 / 50000: loss 29680.761865\n",
      "iteration 12200 / 50000: loss 25841.113297\n",
      "iteration 12300 / 50000: loss 27426.796999\n",
      "iteration 12400 / 50000: loss 38820.015207\n",
      "iteration 12500 / 50000: loss 38963.604201\n",
      "iteration 12600 / 50000: loss 34715.208619\n",
      "iteration 12700 / 50000: loss 36469.647240\n",
      "iteration 12800 / 50000: loss 24337.500842\n",
      "iteration 12900 / 50000: loss 33591.867707\n",
      "iteration 13000 / 50000: loss 36949.963688\n",
      "iteration 13100 / 50000: loss 35798.749040\n",
      "iteration 13200 / 50000: loss 29770.812684\n",
      "iteration 13300 / 50000: loss 29195.761822\n",
      "iteration 13400 / 50000: loss 31404.371012\n",
      "iteration 13500 / 50000: loss 26270.323208\n",
      "iteration 13600 / 50000: loss 33053.413470\n",
      "iteration 13700 / 50000: loss 32980.959108\n",
      "iteration 13800 / 50000: loss 26735.050486\n",
      "iteration 13900 / 50000: loss 43345.718428\n",
      "iteration 14000 / 50000: loss 32050.433414\n",
      "iteration 14100 / 50000: loss 24373.849666\n",
      "iteration 14200 / 50000: loss 31877.203666\n",
      "iteration 14300 / 50000: loss 25248.489268\n",
      "iteration 14400 / 50000: loss 27760.956980\n",
      "iteration 14500 / 50000: loss 38283.485623\n",
      "iteration 14600 / 50000: loss 31512.167187\n",
      "iteration 14700 / 50000: loss 29193.646360\n",
      "iteration 14800 / 50000: loss 37183.708466\n",
      "iteration 14900 / 50000: loss 26483.888991\n",
      "iteration 15000 / 50000: loss 27397.546813\n",
      "iteration 15100 / 50000: loss 36567.860607\n",
      "iteration 15200 / 50000: loss 22061.778018\n",
      "iteration 15300 / 50000: loss 18728.037055\n",
      "iteration 15400 / 50000: loss 27714.316528\n",
      "iteration 15500 / 50000: loss 32958.322352\n",
      "iteration 15600 / 50000: loss 26448.060390\n",
      "iteration 15700 / 50000: loss 26149.705476\n",
      "iteration 15800 / 50000: loss 32646.082633\n",
      "iteration 15900 / 50000: loss 27154.416957\n",
      "iteration 16000 / 50000: loss 31470.152537\n",
      "iteration 16100 / 50000: loss 32001.514287\n",
      "iteration 16200 / 50000: loss 28563.910788\n",
      "iteration 16300 / 50000: loss 31887.056165\n",
      "iteration 16400 / 50000: loss 29705.610582\n",
      "iteration 16500 / 50000: loss 37282.337120\n",
      "iteration 16600 / 50000: loss 25181.471227\n",
      "iteration 16700 / 50000: loss 33905.821779\n",
      "iteration 16800 / 50000: loss 23055.367955\n",
      "iteration 16900 / 50000: loss 30422.595391\n",
      "iteration 17000 / 50000: loss 25223.639515\n",
      "iteration 17100 / 50000: loss 29332.840986\n",
      "iteration 17200 / 50000: loss 28747.307395\n",
      "iteration 17300 / 50000: loss 27938.986528\n",
      "iteration 17400 / 50000: loss 30462.908234\n",
      "iteration 17500 / 50000: loss 33383.587473\n",
      "iteration 17600 / 50000: loss 30491.926745\n",
      "iteration 17700 / 50000: loss 28862.365103\n",
      "iteration 17800 / 50000: loss 34151.513020\n",
      "iteration 17900 / 50000: loss 30310.746685\n",
      "iteration 18000 / 50000: loss 30600.834542\n",
      "iteration 18100 / 50000: loss 28342.575884\n",
      "iteration 18200 / 50000: loss 29141.959828\n",
      "iteration 18300 / 50000: loss 38335.007900\n",
      "iteration 18400 / 50000: loss 27061.217270\n",
      "iteration 18500 / 50000: loss 33478.493997\n",
      "iteration 18600 / 50000: loss 30307.340195\n",
      "iteration 18700 / 50000: loss 26674.265042\n",
      "iteration 18800 / 50000: loss 33310.364723\n",
      "iteration 18900 / 50000: loss 24201.421186\n",
      "iteration 19000 / 50000: loss 36522.489384\n",
      "iteration 19100 / 50000: loss 33906.834676\n",
      "iteration 19200 / 50000: loss 32267.817033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19300 / 50000: loss 25502.834828\n",
      "iteration 19400 / 50000: loss 35374.508432\n",
      "iteration 19500 / 50000: loss 24453.203555\n",
      "iteration 19600 / 50000: loss 27480.425100\n",
      "iteration 19700 / 50000: loss 25739.401765\n",
      "iteration 19800 / 50000: loss 34817.330214\n",
      "iteration 19900 / 50000: loss 30790.293896\n",
      "iteration 20000 / 50000: loss 50936.255772\n",
      "iteration 20100 / 50000: loss 27774.657881\n",
      "iteration 20200 / 50000: loss 36325.119878\n",
      "iteration 20300 / 50000: loss 26682.417342\n",
      "iteration 20400 / 50000: loss 26517.366452\n",
      "iteration 20500 / 50000: loss 28404.404600\n",
      "iteration 20600 / 50000: loss 23678.780047\n",
      "iteration 20700 / 50000: loss 27219.183373\n",
      "iteration 20800 / 50000: loss 28626.753386\n",
      "iteration 20900 / 50000: loss 35552.682291\n",
      "iteration 21000 / 50000: loss 31247.715540\n",
      "iteration 21100 / 50000: loss 28438.229833\n",
      "iteration 21200 / 50000: loss 24482.248822\n",
      "iteration 21300 / 50000: loss 25903.533791\n",
      "iteration 21400 / 50000: loss 27380.746300\n",
      "iteration 21500 / 50000: loss 27371.614226\n",
      "iteration 21600 / 50000: loss 25989.611212\n",
      "iteration 21700 / 50000: loss 41728.294850\n",
      "iteration 21800 / 50000: loss 28213.390699\n",
      "iteration 21900 / 50000: loss 31075.766068\n",
      "iteration 22000 / 50000: loss 33181.819090\n",
      "iteration 22100 / 50000: loss 31919.278681\n",
      "iteration 22200 / 50000: loss 29223.474029\n",
      "iteration 22300 / 50000: loss 23141.016248\n",
      "iteration 22400 / 50000: loss 34661.074827\n",
      "iteration 22500 / 50000: loss 29488.899999\n",
      "iteration 22600 / 50000: loss 28733.274908\n",
      "iteration 22700 / 50000: loss 23325.648704\n",
      "iteration 22800 / 50000: loss 33119.209484\n",
      "iteration 22900 / 50000: loss 46071.755375\n",
      "iteration 23000 / 50000: loss 24950.402578\n",
      "iteration 23100 / 50000: loss 35319.150980\n",
      "iteration 23200 / 50000: loss 31775.541885\n",
      "iteration 23300 / 50000: loss 27628.725833\n",
      "iteration 23400 / 50000: loss 35920.956054\n",
      "iteration 23500 / 50000: loss 33369.388448\n",
      "iteration 23600 / 50000: loss 32471.854248\n",
      "iteration 23700 / 50000: loss 36606.036239\n",
      "iteration 23800 / 50000: loss 32190.129225\n",
      "iteration 23900 / 50000: loss 32960.690522\n",
      "iteration 24000 / 50000: loss 41022.927022\n",
      "iteration 24100 / 50000: loss 44426.764149\n",
      "iteration 24200 / 50000: loss 33104.568075\n",
      "iteration 24300 / 50000: loss 34671.496235\n",
      "iteration 24400 / 50000: loss 36169.645700\n",
      "iteration 24500 / 50000: loss 35519.959144\n",
      "iteration 24600 / 50000: loss 32329.045604\n",
      "iteration 24700 / 50000: loss 34119.533346\n",
      "iteration 24800 / 50000: loss 23626.481455\n",
      "iteration 24900 / 50000: loss 37882.698730\n",
      "iteration 25000 / 50000: loss 27951.657616\n",
      "iteration 25100 / 50000: loss 28381.954887\n",
      "iteration 25200 / 50000: loss 28761.600530\n",
      "iteration 25300 / 50000: loss 27742.726797\n",
      "iteration 25400 / 50000: loss 29775.171525\n",
      "iteration 25500 / 50000: loss 27620.737751\n",
      "iteration 25600 / 50000: loss 34965.653380\n",
      "iteration 25700 / 50000: loss 31691.077699\n",
      "iteration 25800 / 50000: loss 30940.766038\n",
      "iteration 25900 / 50000: loss 29295.831693\n",
      "iteration 26000 / 50000: loss 37159.593877\n",
      "iteration 26100 / 50000: loss 29007.619257\n",
      "iteration 26200 / 50000: loss 30129.694347\n",
      "iteration 26300 / 50000: loss 27357.227159\n",
      "iteration 26400 / 50000: loss 30749.115428\n",
      "iteration 26500 / 50000: loss 27867.276541\n",
      "iteration 26600 / 50000: loss 26258.761578\n",
      "iteration 26700 / 50000: loss 29261.130793\n",
      "iteration 26800 / 50000: loss 30074.242329\n",
      "iteration 26900 / 50000: loss 27806.560692\n",
      "iteration 27000 / 50000: loss 38754.173914\n",
      "iteration 27100 / 50000: loss 24311.501835\n",
      "iteration 27200 / 50000: loss 32273.969551\n",
      "iteration 27300 / 50000: loss 31770.290071\n",
      "iteration 27400 / 50000: loss 27958.253092\n",
      "iteration 27500 / 50000: loss 27650.161314\n",
      "iteration 27600 / 50000: loss 37572.552306\n",
      "iteration 27700 / 50000: loss 25165.372860\n",
      "iteration 27800 / 50000: loss 36331.985821\n",
      "iteration 27900 / 50000: loss 37588.900597\n",
      "iteration 28000 / 50000: loss 27036.208562\n",
      "iteration 28100 / 50000: loss 41147.924838\n",
      "iteration 28200 / 50000: loss 26287.809187\n",
      "iteration 28300 / 50000: loss 27376.571412\n",
      "iteration 28400 / 50000: loss 28303.598894\n",
      "iteration 28500 / 50000: loss 27685.969866\n",
      "iteration 28600 / 50000: loss 25750.822229\n",
      "iteration 28700 / 50000: loss 37851.520510\n",
      "iteration 28800 / 50000: loss 27835.744354\n",
      "iteration 28900 / 50000: loss 27529.971967\n",
      "iteration 29000 / 50000: loss 38564.464655\n",
      "iteration 29100 / 50000: loss 25498.945939\n",
      "iteration 29200 / 50000: loss 26039.432507\n",
      "iteration 29300 / 50000: loss 26841.432829\n",
      "iteration 29400 / 50000: loss 27056.502188\n",
      "iteration 29500 / 50000: loss 37122.743544\n",
      "iteration 29600 / 50000: loss 33506.167595\n",
      "iteration 29700 / 50000: loss 33280.678923\n",
      "iteration 29800 / 50000: loss 28670.311549\n",
      "iteration 29900 / 50000: loss 29338.852632\n",
      "iteration 30000 / 50000: loss 31679.044904\n",
      "iteration 30100 / 50000: loss 29295.880651\n",
      "iteration 30200 / 50000: loss 26004.723245\n",
      "iteration 30300 / 50000: loss 38724.601429\n",
      "iteration 30400 / 50000: loss 30385.485150\n",
      "iteration 30500 / 50000: loss 28378.403320\n",
      "iteration 30600 / 50000: loss 28134.868213\n",
      "iteration 30700 / 50000: loss 30368.070557\n",
      "iteration 30800 / 50000: loss 31086.542384\n",
      "iteration 30900 / 50000: loss 29852.765460\n",
      "iteration 31000 / 50000: loss 27838.541026\n",
      "iteration 31100 / 50000: loss 25670.087797\n",
      "iteration 31200 / 50000: loss 32180.946144\n",
      "iteration 31300 / 50000: loss 29294.861736\n",
      "iteration 31400 / 50000: loss 28753.086263\n",
      "iteration 31500 / 50000: loss 22387.719556\n",
      "iteration 31600 / 50000: loss 39186.792554\n",
      "iteration 31700 / 50000: loss 33415.079776\n",
      "iteration 31800 / 50000: loss 36923.867723\n",
      "iteration 31900 / 50000: loss 30406.276754\n",
      "iteration 32000 / 50000: loss 27612.221694\n",
      "iteration 32100 / 50000: loss 28154.618462\n",
      "iteration 32200 / 50000: loss 32429.376022\n",
      "iteration 32300 / 50000: loss 24073.805498\n",
      "iteration 32400 / 50000: loss 34254.370953\n",
      "iteration 32500 / 50000: loss 30329.204104\n",
      "iteration 32600 / 50000: loss 31532.994529\n",
      "iteration 32700 / 50000: loss 30129.341579\n",
      "iteration 32800 / 50000: loss 29602.301266\n",
      "iteration 32900 / 50000: loss 30088.917103\n",
      "iteration 33000 / 50000: loss 35629.869629\n",
      "iteration 33100 / 50000: loss 29213.048729\n",
      "iteration 33200 / 50000: loss 33868.482729\n",
      "iteration 33300 / 50000: loss 30033.799724\n",
      "iteration 33400 / 50000: loss 29065.567479\n",
      "iteration 33500 / 50000: loss 29387.688517\n",
      "iteration 33600 / 50000: loss 33008.136705\n",
      "iteration 33700 / 50000: loss 30912.641489\n",
      "iteration 33800 / 50000: loss 27055.679879\n",
      "iteration 33900 / 50000: loss 23752.842520\n",
      "iteration 34000 / 50000: loss 25750.529557\n",
      "iteration 34100 / 50000: loss 29627.482442\n",
      "iteration 34200 / 50000: loss 25497.904737\n",
      "iteration 34300 / 50000: loss 27289.934635\n",
      "iteration 34400 / 50000: loss 23914.372273\n",
      "iteration 34500 / 50000: loss 31098.354553\n",
      "iteration 34600 / 50000: loss 22623.457136\n",
      "iteration 34700 / 50000: loss 33332.111285\n",
      "iteration 34800 / 50000: loss 27512.092362\n",
      "iteration 34900 / 50000: loss 34958.089863\n",
      "iteration 35000 / 50000: loss 31692.725070\n",
      "iteration 35100 / 50000: loss 27949.191614\n",
      "iteration 35200 / 50000: loss 39192.351551\n",
      "iteration 35300 / 50000: loss 31774.278988\n",
      "iteration 35400 / 50000: loss 35670.243240\n",
      "iteration 35500 / 50000: loss 28750.836671\n",
      "iteration 35600 / 50000: loss 34968.339421\n",
      "iteration 35700 / 50000: loss 45858.184321\n",
      "iteration 35800 / 50000: loss 28255.089935\n",
      "iteration 35900 / 50000: loss 27939.932402\n",
      "iteration 36000 / 50000: loss 34123.194966\n",
      "iteration 36100 / 50000: loss 29154.704541\n",
      "iteration 36200 / 50000: loss 27565.503882\n",
      "iteration 36300 / 50000: loss 29746.945418\n",
      "iteration 36400 / 50000: loss 28929.998759\n",
      "iteration 36500 / 50000: loss 28177.826306\n",
      "iteration 36600 / 50000: loss 35460.947319\n",
      "iteration 36700 / 50000: loss 27615.626486\n",
      "iteration 36800 / 50000: loss 34199.707210\n",
      "iteration 36900 / 50000: loss 28360.563038\n",
      "iteration 37000 / 50000: loss 23624.305510\n",
      "iteration 37100 / 50000: loss 31137.579902\n",
      "iteration 37200 / 50000: loss 37207.167413\n",
      "iteration 37300 / 50000: loss 38031.087303\n",
      "iteration 37400 / 50000: loss 27545.345811\n",
      "iteration 37500 / 50000: loss 25282.140814\n",
      "iteration 37600 / 50000: loss 26086.092360\n",
      "iteration 37700 / 50000: loss 31687.971876\n",
      "iteration 37800 / 50000: loss 37586.410005\n",
      "iteration 37900 / 50000: loss 19648.896653\n",
      "iteration 38000 / 50000: loss 26974.946687\n",
      "iteration 38100 / 50000: loss 34125.356781\n",
      "iteration 38200 / 50000: loss 27450.759698\n",
      "iteration 38300 / 50000: loss 26218.489787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 38400 / 50000: loss 29587.361658\n",
      "iteration 38500 / 50000: loss 32529.065356\n",
      "iteration 38600 / 50000: loss 36043.483563\n",
      "iteration 38700 / 50000: loss 31360.514912\n",
      "iteration 38800 / 50000: loss 34531.489324\n",
      "iteration 38900 / 50000: loss 34315.281410\n",
      "iteration 39000 / 50000: loss 29752.326085\n",
      "iteration 39100 / 50000: loss 22456.579883\n",
      "iteration 39200 / 50000: loss 29149.510369\n",
      "iteration 39300 / 50000: loss 22727.754888\n",
      "iteration 39400 / 50000: loss 27094.105787\n",
      "iteration 39500 / 50000: loss 22245.346553\n",
      "iteration 39600 / 50000: loss 28931.644017\n",
      "iteration 39700 / 50000: loss 30673.185221\n",
      "iteration 39800 / 50000: loss 38195.713337\n",
      "iteration 39900 / 50000: loss 27154.815175\n",
      "iteration 40000 / 50000: loss 25142.198913\n",
      "iteration 40100 / 50000: loss 37685.036742\n",
      "iteration 40200 / 50000: loss 36676.728734\n",
      "iteration 40300 / 50000: loss 29254.774175\n",
      "iteration 40400 / 50000: loss 32005.677007\n",
      "iteration 40500 / 50000: loss 27360.635622\n",
      "iteration 40600 / 50000: loss 27899.159934\n",
      "iteration 40700 / 50000: loss 36032.447060\n",
      "iteration 40800 / 50000: loss 31879.102832\n",
      "iteration 40900 / 50000: loss 28608.775191\n",
      "iteration 41000 / 50000: loss 41313.594994\n",
      "iteration 41100 / 50000: loss 32098.477054\n",
      "iteration 41200 / 50000: loss 28539.074311\n",
      "iteration 41300 / 50000: loss 28936.455713\n",
      "iteration 41400 / 50000: loss 33480.592834\n",
      "iteration 41500 / 50000: loss 38536.593322\n",
      "iteration 41600 / 50000: loss 33833.361101\n",
      "iteration 41700 / 50000: loss 35093.469876\n",
      "iteration 41800 / 50000: loss 28696.308953\n",
      "iteration 41900 / 50000: loss 37707.328978\n",
      "iteration 42000 / 50000: loss 30197.820386\n",
      "iteration 42100 / 50000: loss 24131.054903\n",
      "iteration 42200 / 50000: loss 35748.690775\n",
      "iteration 42300 / 50000: loss 27394.670834\n",
      "iteration 42400 / 50000: loss 25668.014276\n",
      "iteration 42500 / 50000: loss 31372.324750\n",
      "iteration 42600 / 50000: loss 28865.363822\n",
      "iteration 42700 / 50000: loss 36127.749352\n",
      "iteration 42800 / 50000: loss 27979.335024\n",
      "iteration 42900 / 50000: loss 34707.864391\n",
      "iteration 43000 / 50000: loss 28011.870060\n",
      "iteration 43100 / 50000: loss 28831.608513\n",
      "iteration 43200 / 50000: loss 30791.958441\n",
      "iteration 43300 / 50000: loss 32388.924674\n",
      "iteration 43400 / 50000: loss 27855.486080\n",
      "iteration 43500 / 50000: loss 24734.889301\n",
      "iteration 43600 / 50000: loss 28455.742163\n",
      "iteration 43700 / 50000: loss 26747.698508\n",
      "iteration 43800 / 50000: loss 27322.799613\n",
      "iteration 43900 / 50000: loss 30986.460432\n",
      "iteration 44000 / 50000: loss 31088.372343\n",
      "iteration 44100 / 50000: loss 31976.610484\n",
      "iteration 44200 / 50000: loss 34486.160954\n",
      "iteration 44300 / 50000: loss 40291.159238\n",
      "iteration 44400 / 50000: loss 28178.213430\n",
      "iteration 44500 / 50000: loss 30508.341310\n",
      "iteration 44600 / 50000: loss 28457.843735\n",
      "iteration 44700 / 50000: loss 35384.151842\n",
      "iteration 44800 / 50000: loss 28388.878517\n",
      "iteration 44900 / 50000: loss 34777.126451\n",
      "iteration 45000 / 50000: loss 37745.837221\n",
      "iteration 45100 / 50000: loss 34774.229017\n",
      "iteration 45200 / 50000: loss 23957.006382\n",
      "iteration 45300 / 50000: loss 35962.783055\n",
      "iteration 45400 / 50000: loss 24074.503077\n",
      "iteration 45500 / 50000: loss 28699.783801\n",
      "iteration 45600 / 50000: loss 28710.405675\n",
      "iteration 45700 / 50000: loss 28715.040729\n",
      "iteration 45800 / 50000: loss 27766.633569\n",
      "iteration 45900 / 50000: loss 40215.294115\n",
      "iteration 46000 / 50000: loss 26740.269233\n",
      "iteration 46100 / 50000: loss 28719.250118\n",
      "iteration 46200 / 50000: loss 31571.539945\n",
      "iteration 46300 / 50000: loss 35348.572378\n",
      "iteration 46400 / 50000: loss 32971.218146\n",
      "iteration 46500 / 50000: loss 26005.362715\n",
      "iteration 46600 / 50000: loss 34236.236918\n",
      "iteration 46700 / 50000: loss 36136.404523\n",
      "iteration 46800 / 50000: loss 25448.111729\n",
      "iteration 46900 / 50000: loss 32009.678918\n",
      "iteration 47000 / 50000: loss 27857.295040\n",
      "iteration 47100 / 50000: loss 30874.011043\n",
      "iteration 47200 / 50000: loss 20972.937025\n",
      "iteration 47300 / 50000: loss 31780.321468\n",
      "iteration 47400 / 50000: loss 24490.037699\n",
      "iteration 47500 / 50000: loss 36765.924637\n",
      "iteration 47600 / 50000: loss 27804.460521\n",
      "iteration 47700 / 50000: loss 31706.534829\n",
      "iteration 47800 / 50000: loss 27943.432259\n",
      "iteration 47900 / 50000: loss 33401.555251\n",
      "iteration 48000 / 50000: loss 28385.379318\n",
      "iteration 48100 / 50000: loss 29125.078210\n",
      "iteration 48200 / 50000: loss 26060.564229\n",
      "iteration 48300 / 50000: loss 28062.698713\n",
      "iteration 48400 / 50000: loss 26745.713740\n",
      "iteration 48500 / 50000: loss 31003.223446\n",
      "iteration 48600 / 50000: loss 29748.654683\n",
      "iteration 48700 / 50000: loss 34487.246331\n",
      "iteration 48800 / 50000: loss 33904.614047\n",
      "iteration 48900 / 50000: loss 40127.977687\n",
      "iteration 49000 / 50000: loss 32742.793010\n",
      "iteration 49100 / 50000: loss 32009.341150\n",
      "iteration 49200 / 50000: loss 27011.972731\n",
      "iteration 49300 / 50000: loss 27974.319780\n",
      "iteration 49400 / 50000: loss 29400.805694\n",
      "iteration 49500 / 50000: loss 36596.889755\n",
      "iteration 49600 / 50000: loss 36850.838869\n",
      "iteration 49700 / 50000: loss 33171.288230\n",
      "iteration 49800 / 50000: loss 23961.999980\n",
      "iteration 49900 / 50000: loss 38739.192702\n",
      "Validation error:  1243.04065949\n"
     ]
    }
   ],
   "source": [
    "# Now, let's train a neural network\n",
    "\n",
    "input_size = input_size\n",
    "hidden_size = 500 # TODO: Choose a suitable hidden layer size\n",
    "num_classes = 2 # We have two outputs\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=50000, batch_size=64,\n",
    "            learning_rate=1e-5, learning_rate_decay=0.95,\n",
    "            reg=0.5, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "#val_err = ... # TODO: Perform prediction on the validation set\n",
    "val_err = np.sum(np.square(net.predict(X_val) - y_val), axis=1).mean()\n",
    "print ('Validation error: ', val_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug the training and improve learning\n",
    "You should be able to get a validation error of 5.\n",
    "\n",
    "So far so good. But, is it really good? Let us plot the validation and training errors to see how good the network did. Did it memorize or generalize? Discuss your observations and conclusions. If its performance is not looking good, propose and test measures. This is the part that will show me how well you have digested everything covered in the lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAHwCAYAAAA2B95/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f41c9b60048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss function and train / validation errors\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "train = plt.plot(stats['train_err_history'], label='train')\n",
    "val = plt.plot(stats['val_err_history'], label='val')\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.title('Classification error history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19150.281411869848]\n",
      "739 23669\n"
     ]
    }
   ],
   "source": [
    "print(stats['train_err_history'])\n",
    "iterations_per_epoch = int(max(X_train.shape[0] / 32, 1))\n",
    "print(iterations_per_epoch, X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning and Improving Your Network (Bonus)\n",
    "There are many aspects and hyper-parameters you can play with. Do play with them and find the best setting here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
